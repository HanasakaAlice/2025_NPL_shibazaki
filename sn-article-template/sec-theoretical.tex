\section{Appendix} \label{sec:appendix}

\subsection{Theoretical Analysis} \label{ch:theory}

In this chapter, we consider a binary classification problem where input data may be subject to adversarial perturbations, and derive upper bounds on the gap between the true adversarial risk and its empirical estimator for both supervised learning and PU learning. By comparing these bounds, we theoretically clarify conditions under which PU learning can be advantageous over supervised learning in the finite-sample regime.

Below we define the problem setting used throughout this chapter.

\begin{problem}{(Adversarial Binary Classification Setting)}
\label{problem_adv}

Let the input space be $\mathcal{X} \subseteq \mathbb{R}^d$,
and the label space be $\mathcal{Y}=\{-1, +1\}$.
Assume the input $\boldsymbol{x} \in \mathcal{X}$ is bounded,
i.e., there exists a constant $C_x>0$ such that
\[
\|\boldsymbol{x}\|_\infty \leq C_x
\]
holds.

Assume $(\boldsymbol{x},y)$ is generated from a joint distribution $p(\boldsymbol{x},y)$, and define the class-conditional distributions as

\[
p_{\mathrm{P}}(\boldsymbol{x}) = p(\boldsymbol{x}\mid y=+1), \quad
p_{\mathrm{N}}(\boldsymbol{x}) = p(\boldsymbol{x}\mid y=-1)
\]
as above.
Let the class prior probabilities be
$\pi_{\mathrm{P}} = p(y=+1)$，
$\pi_{\mathrm{N}} = p(y=-1)$
and let
with $\pi_{\mathrm{P}}+\pi_{\mathrm{N}}=1$.

In supervised learning, we use labeled data from the positive (P) and negative (N) classes.
The corresponding datasets are
\begin{equation}
\begin{aligned}
\mathscr{X}_{\mathrm{P}} &= \{\boldsymbol{x}_i^{\mathrm{P}}\}_{i=1}^{n_{\mathrm{P}}}
\stackrel{\mathrm{i.i.d.}}{\sim} p_{\mathrm{P}}(\boldsymbol{x}),\\
\mathscr{X}_{\mathrm{N}} &= \{\boldsymbol{x}_i^{\mathrm{N}}\}_{i=1}^{n_{\mathrm{N}}}
\stackrel{\mathrm{i.i.d.}}{\sim} p_{\mathrm{N}}(\boldsymbol{x})
\end{aligned}
\end{equation}
given by

In PU learning, we use positive data and unlabeled (U) data.
The marginal distribution of unlabeled data is
\[
p_{\mathrm{U}}(\boldsymbol{x})
= \pi_{\mathrm{P}} p_{\mathrm{P}}(\boldsymbol{x})
+ \pi_{\mathrm{N}} p_{\mathrm{N}}(\boldsymbol{x})
\]
given by
and the unlabeled dataset is
\begin{equation}
\mathscr{X}_{\mathrm{U}}
= \{\boldsymbol{x}_i^{\mathrm{U}}\}_{i=1}^{n_{\mathrm{U}}}
\stackrel{\mathrm{i.i.d.}}{\sim} p_{\mathrm{U}}(\boldsymbol{x})
\end{equation}
 .

Let the classifier be $g:\mathbb{R}^d \rightarrow \mathbb{R}$, and
consider a linear classifier parameterized by a weight vector $\boldsymbol{w}\in\mathbb{R}^d$:
\[
g(\boldsymbol{x}) = \boldsymbol{w}^\top \boldsymbol{x}
\]

For $p\ge1$ and $W>0$, define the hypothesis class as
\[
\mathscr{G}
= \{ g(\boldsymbol{x}): \|\boldsymbol{w}\|_p \le W \}
\]
as above.

Let the loss function $\ell:\mathbb{R}\times\mathcal{Y}\rightarrow\mathbb{R}$ take as arguments the classifier output and the true label.

Moreover, as the adversarial regularization term in TRADES we use
\[
\ell_{\mathrm{KL}}\big(g(\bm x),g(\bm x')\big)
\]
where $\ell_{\mathrm{KL}}$ denotes
the Kullback--Leibler divergence between probability distributions induced by the classifier outputs.

Throughout this chapter, we assume that $\ell$ and $\ell_{\mathrm{KL}}$ satisfy the regularity conditions needed for the analysis (boundedness and Lipschitz continuity); see the assumptions in Theorem\ref{thm:pntr-estimation} for details.
\end{problem}


\subsection{Preliminaries (Notation and Assumptions)}\label{subsec:theory_tools}

To derive the estimation-error upper bounds in this chapter, we use the Rademacher complexity to bound the expected uniform deviation,
and we use McDiarmid's inequality to obtain high-probability guarantees that hold with probability at least $1-\delta$.
Below we summarize the definitions and properties used in this chapter.

\paragraph{Rademacher Complexity}\label{rad}
In our discussion, we need a measure of the complexity of a function class $\mathscr{G}$.
We adopt the Rademacher complexity and recall its definition.
A random variable $\sigma$ satisfying $\Pr(\sigma=+1)=\Pr(\sigma=-1)=1/2$ is called a Rademacher variable.
Given a set of $n$-dimensional vectors $S\subseteq\mathbb{R}^n$,
the Rademacher complexity of $S$ is defined as
\[
\mathfrak{R}(S)
=\mathbb{E}_{\sigma_1,\ldots,\sigma_n}\left[
\sup_{(s_1,\ldots,s_n)\in S}\frac{1}{n}\sum_{i=1}^n \sigma_i s_i
\right]
\]
as above.
It can be interpreted as the expected correlation between random noise and the best-matching element of $S$.

Next, let $S_n=\{\bm{x}_i\}_{i=1}^n$ be a dataset of size $n$.
For a function class $\mathscr{G}$, define
\[
\mathscr{G}\circ S_n
=\left\{\left(g(\bm{x}_1),\ldots,g(\bm{x}_n)\right)\ \middle|\ g\in\mathscr{G}\right\}
\]
Then,
the empirical Rademacher complexity of $\mathscr{G}$ on $S_n$ is
\[
\mathfrak{R}_{S_n}(\mathscr{G})
=\mathfrak{R}(\mathscr{G}\circ S_n)
=\mathbb{E}_{\sigma_1,\ldots,\sigma_n}\left[
\sup_{g\in\mathscr{G}}\frac{1}{n}\sum_{i=1}^n \sigma_i g(\bm{x}_i)
\right]
\]
given by
Furthermore, when $S_n=\{\bm{x}_i\}_{i=1}^n \stackrel{\text{i.i.d.}}{\sim} \nu(\bm{x})$, we define the Rademacher complexity of $\mathscr{G}$ as follows (where $\nu(\bm{x})$ denotes the distribution of $\bm{x}$).

\begin{tcolorbox}[
    colback = white,
    colframe = black!60,
    boxrule = 0.35pt,
    fonttitle = \bfseries,
    breakable = false]
\begin{dfn}[Rademacher Complexity]\label{thm:def_rademacher}
Let $n$ be the sample size and let $S_n=\{\bm{x}_i\}_{i=1}^n \stackrel{\text{i.i.d.}}{\sim} \nu(\bm{x})$.
Then the Rademacher complexity of $\mathscr{G}$ is
\begin{equation}
 \mathfrak{R}_{n, \nu}(\mathscr{G})
 =\mathbb{E}_{S_n \sim \nu^n}\left[\mathfrak{R}_{S_n}(\mathscr{G})\right]
 =\mathbb{E}_{\bm{x}_1,\ldots,\bm{x}_n}
 \mathbb{E}_{\sigma_1,\ldots,\sigma_n}\left[
 \sup_{g \in \mathscr{G}} \frac{1}{n} \sum_{i=1}^n \sigma_i g(\bm{x}_i)
 \right]
\label{eq:emprad}
\end{equation}
is defined as above.
\end{dfn}
\end{tcolorbox}

\par\noindent
Standard results used in this chapter (Talagrand's contraction lemma, vector contraction, Rademacher complexity bounds for linear function classes, additional bounds for adversarial inputs, McDiarmid's inequality, etc.) are summarized in Appendix\ref{app:toolbox}.

\par\noindent\textit{(Proofs of theorems and lemmas are provided in Appendix~\ref{app:theory-proofs}.)}

\subsection{Upper Bound on the Estimation Error of Supervised TRADES}

In this section, we derive an upper bound on the estimation error of supervised TRADES.
The proof proceeds by (i) deriving an upper bound on the uniform deviation (lemma),
and (ii) applying the standard ERM argument to obtain the estimation-error bound (theorem),
following the standard flow.

\paragraph{Supervised TRADES risk (population risk and empirical risk)}
We define the population risk of supervised TRADES as
\begin{equation}
\begin{aligned}
R_{\mathrm{PN\mbox{-}TR}}(g)
:=
&\ \pi_{\mathrm P}\mathbb{E}_{\mathrm P}\!\left[
\ell\big(g(\bm x),+1\big)+\beta\max_{\|\bm\eta\|_\infty\le\epsilon}
\ell_{\mathrm{KL}}\big(g(\bm x),g(\bm x+\bm\eta)\big)
\right]
\\
&+\pi_{\mathrm N}\mathbb{E}_{\mathrm N}\!\left[
\ell\big(g(\bm x),-1\big)+\beta\max_{\|\bm\eta\|_\infty\le\epsilon}
\ell_{\mathrm{KL}}\big(g(\bm x),g(\bm x+\bm\eta)\big)
\right]
\label{eq:pntr-risk-jp}
\end{aligned}
\end{equation}
as above.
The corresponding empirical risk is
\begin{equation}
\begin{aligned}
\widehat R_{\mathrm{PN\mbox{-}TR}}(g)
:=
&\ \frac{\pi_{\mathrm P}}{n_{\mathrm P}}\sum_{i=1}^{n_{\mathrm P}}
\left[
\ell\big(g(\bm x_i^{\mathrm P}),+1\big)+\beta\max_{\|\bm\eta\|_\infty\le\epsilon}
\ell_{\mathrm{KL}}\big(g(\bm x_i^{\mathrm P}),g(\bm x_i^{\mathrm P}+\bm\eta)\big)
\right]
\\
&+\frac{\pi_{\mathrm N}}{n_{\mathrm N}}\sum_{i=1}^{n_{\mathrm N}}
\left[
\ell\big(g(\bm x_i^{\mathrm N}),-1\big)+\beta\max_{\|\bm\eta\|_\infty\le\epsilon}
\ell_{\mathrm{KL}}\big(g(\bm x_i^{\mathrm N}),g(\bm x_i^{\mathrm N}+\bm\eta)\big)
\right]
\label{eq:pntr-emp-jp}
\end{aligned}
\end{equation}
 .
Define the empirical risk minimizer as
\[
\widehat g_{\mathrm{PN\mbox{-}TR}}
:=\arg\min_{g\in\mathscr G}\widehat R_{\mathrm{PN\mbox{-}TR}}(g)
\]
Also, let
\[
g^*\in\arg\min_{g\in\mathscr G} R_{\mathrm{PN\mbox{-}TR}}(g)
\]
be a population risk minimizer.
With these definitions, we obtain the following upper bound on the estimation error of supervised TRADES.




\begin{tcolorbox}[
    colback = white,
    colframe = black,
    fonttitle = \bfseries,
    breakable = false]
\begin{thm}[Upper Bound on the Estimation Error of Supervised TRADES]
\label{thm:pntr-estimation}

Let a function class $\mathscr{G}$ be given.
Assume the following:

\begin{itemize}
\item \textbf{(Boundedness of the loss)}
One of the following holds:
\begin{itemize}
\item There exists a constant $C_\ell>0$ such that
for any $\widehat y\in\mathbb{R}$ and $y\in\mathcal{Y}$,
$\ell(\widehat y,y)\le C_\ell$ holds; or
\item there exists a constant $C_g>0$ such that
$\|g\|_\infty=\sup_{\bm x\in\mathcal X}|g(\bm x)|\le C_g$
holds for any $g\in\mathscr{G}$,
and for $|\widehat y|\le C_g$,
$\ell(\widehat y,y)\le C_\ell$ holds.
\end{itemize}

\item \textbf{(Lipschitz continuity of the loss)}
$\ell(\widehat y,y)$ is
$L_\ell$-Lipschitz continuous with respect to $\widehat y$.

\item \textbf{(Regularity of the KL term)}
The TRADES regularization term $\ell_{\mathrm{KL}}\left(g(\boldsymbol{x}), g\left(\boldsymbol{x}^{\prime}\right)\right)$
is $L_{\mathrm{KL}}$-Lipschitz continuous in each argument,
and is uniformly bounded:
$\ell_{\mathrm{KL}}(u,v)\le C_{\mathrm{KL}}$
holds.
\end{itemize}

Then, for any $\delta>0$,
the following holds with probability at least $1-\delta$:
\small
\begin{equation}
\begin{aligned}
R_{\mathrm{PN\mbox{-}TR}}(\widehat g_{\mathrm{PN\mbox{-}TR}})
-
R_{\mathrm{PN\mbox{-}TR}}(g^*)
\;\le\;
&
4\big(L_\ell+4\beta L_{\mathrm{KL}}\big)
\Big(
\pi_{\mathrm P}\mathfrak{R}_{n_{\mathrm P},p_{\mathrm P}}(\mathscr{G})
+
\pi_{\mathrm N}\mathfrak{R}_{n_{\mathrm N},p_{\mathrm N}}(\mathscr{G})
\Big)
\\
&+
8\beta L_{\mathrm{KL}}\,\varepsilon W d^{1/q}\left(\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}+\frac{\pi_{\mathrm N}}{\sqrt{n_{\mathrm N}}}\right)
\\
&+
\sqrt{2\ln\frac{2}{\delta}}\,
(C_\ell+\beta C_{\mathrm{KL}})
\left(
\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}
+
\frac{\pi_{\mathrm N}}{\sqrt{n_{\mathrm N}}}
\right).
\end{aligned}
\end{equation}
\end{thm}
\end{tcolorbox}

\paragraph{\textbf{Interpretation and implications (statistical convergence rate)}}
Under the linear-in-parameters model ($\|\bm w\|_p\le W$) and bounded inputs, standard bounds in Appendix~\ref{app:toolbox} yield
\[
\mathfrak{R}_{n_{\mathrm P}, p_{\mathrm P}}(\mathscr G)=\mathcal{O}\!\left(\frac{W}{\sqrt{n_{\mathrm P}}}\right),\qquad
\mathfrak{R}_{n_{\mathrm N}, p_{\mathrm N}}(\mathscr G)=\mathcal{O}\!\left(\frac{W}{\sqrt{n_{\mathrm N}}}\right)
\]
Substituting this bound into Theorem~\ref{thm:pntr-estimation} and absorbing constants such as $\beta$ and $\varepsilon$, the estimation-error upper bound becomes
\[
R_{\mathrm{PN\mbox{-}TR}}(\widehat g_{\mathrm{PN\mbox{-}TR}})-R_{\mathrm{PN\mbox{-}TR}}(g^*)
=
\mathcal{O}_p\!\left(\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}+\frac{\pi_{\mathrm N}}{\sqrt{n_{\mathrm N}}}\right)
\]
This implies an $\mathcal{O}_p\!\left(\pi_{\mathrm P}/\sqrt{n_{\mathrm P}}+\pi_{\mathrm N}/\sqrt{n_{\mathrm N}}\right)$ rate, and in particular the bound converges to $0$ in probability as $n_{\mathrm P},n_{\mathrm N}\to\infty$.

As preparation for Theorem~\ref{thm:pntr-estimation}, we first bound the uniform deviation for supervised TRADES.


\paragraph{Auxiliary lemmas}
In what follows, to bound the Rademacher complexity terms arising from the adversarial component of TRADES,
we use two auxiliary lemmas: Lemma~\ref{lem:advRad} and Lemma~\ref{lem:vecContract}.
(Hereafter, $q$ denotes the conjugate exponent of $p$ (i.e., $1/p+1/q=1$).)




% \begin{proof}
% This can be shown by applying $\ell(u,v)-\ell(u',v')\le L_\ell|u-u'|+L_\ell|v-v'|$ to the Rademacher sum.
% \end{proof}

\begin{tcolorbox}[
    colback = white,
    colframe = black!60,
    boxrule = 0.35pt,
    fonttitle = \bfseries,
    breakable = false]
\begin{lem}[Upper Bound on the Uniform Deviation for Supervised TRADES]
\label{lem:pntr-unif-jp}
Under the above assumptions, for any $\delta>0$, with probability at least $1-\delta$,
\begin{equation}
\begin{aligned}
\sup_{g\in\mathscr G}\left|
\widehat R_{\mathrm{PN\mbox{-}TR}}(g)-R_{\mathrm{PN\mbox{-}TR}}(g)
\right|
\le\;
&
2\bigl(L_\ell+4\beta L_{\mathrm{KL}}\bigr)\Big(\pi_{\mathrm P}\mathfrak{R}_{n_{\mathrm P},p_{\mathrm P}}(\mathscr G)
+
\pi_{\mathrm N}\mathfrak{R}_{n_{\mathrm N},p_{\mathrm N}}(\mathscr G)\Big)
\\
&+
4\beta L_{\mathrm{KL}}\,\varepsilon W d^{1/q}\left(\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}+\frac{\pi_{\mathrm N}}{\sqrt{n_{\mathrm N}}}\right)
\\
&+
\sqrt{\frac{1}{2}\ln\frac{2}{\delta}}\,(C_\ell+\beta C_{\mathrm{KL}})
\left(\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}+\frac{\pi_{\mathrm N}}{\sqrt{n_{\mathrm N}}}\right).
\end{aligned}
\label{eq:pntr-unif-bound-jp}
\end{equation}
holds.
\end{lem}
\end{tcolorbox}


\par\noindent\textit{(Proof is given in Appendix~\ref{app:proof-lem-pntr-unif-jp}.)}




\par\noindent\textit{(Proof is given in Appendix~\ref{app:proof-thm-pntr-estimation}.)}


%============================================================
% uPU+TRADES
%============================================================



\subsection{Upper Bound on the Estimation Error of uPU+TRADES}

In this section, we consider the uPU+TRADES objective based on positive (P) and unlabeled (U) data,
and derive an upper bound on its estimation error.

\paragraph{uPU+TRADES risk (population risk and empirical risk)}
Using the composite loss $\tilde{\ell}(g(\boldsymbol{x}), y)=\ell(g(\boldsymbol{x}), y)-\ell(g(\boldsymbol{x}),-y)$
we define the population risk of uPU+TRADES as
\begin{align}
R_{\mathrm{uPU\mbox{-}TR}}(g)
:=
&\ \pi_{\mathrm P}\mathbb{E}_{\mathrm P}\!\left[
\tilde{\ell}(g(\boldsymbol{x}), +1)
+\beta\max_{\|\bm\eta\|_\infty\le\epsilon}
\ell_{\mathrm{KL}}\big(g(\bm x+\bm\eta),g(\bm x)\big)
\right]
\nonumber\\
&+\mathbb{E}_{\mathrm U}\!\left[
\ell\big(g(\bm x),-1\big)+\beta\max_{\|\bm\eta\|_\infty\le\epsilon}
\ell_{\mathrm{KL}}\big(g(\bm x+\bm\eta),g(\bm x)\big)
\right]
\label{eq:uputr-risk-jp}
\end{align}
as above.
The corresponding empirical risk is
\begin{align}
\widehat R_{\mathrm{uPU\mbox{-}TR}}(g)
:=
&\ \frac{\pi_{\mathrm P}}{n_{\mathrm P}}\sum_{i=1}^{n_{\mathrm P}}
\left[
\tilde{\ell}(g(\boldsymbol{x}_i^{\mathrm P}), +1)
+\beta\max_{\|\bm\eta\|_\infty\le\epsilon}
\ell_{\mathrm{KL}}\big(g(\bm x_i^{\mathrm P}+\bm\eta),g(\bm x_i^{\mathrm P})\big)
\right]
\nonumber\\
&+\frac{1}{n_{\mathrm U}}\sum_{i=1}^{n_{\mathrm U}}
\left[
\ell\big(g(\bm x_i^{\mathrm U}),-1\big)
+\beta\max_{\|\bm\eta\|_\infty\le\epsilon}
\ell_{\mathrm{KL}}\big(g(\bm x_i^{\mathrm U}+\bm\eta),g(\bm x_i^{\mathrm U})\big)
\right]
\label{eq:uputr-emp-jp}
\end{align}
 .
Define the empirical risk minimizer as
\[
\widehat g_{\mathrm{uPU\mbox{-}TR}}
:=\arg\min_{g\in\mathscr G}\widehat R_{\mathrm{uPU\mbox{-}TR}}(g)
\]
Also, let
\[
g^*\in\arg\min_{g\in\mathscr G} R_{\mathrm{uPU\mbox{-}TR}}(g)
\]
be a population risk minimizer.


\begin{tcolorbox}[
    colback = white,
    colframe = black,
    fonttitle = \bfseries,
    breakable = false,
  %   left=0mm,right=0mm,  % remove left/right padding
  % top=1mm,bottom=1mm
  ]
\begin{thm}[Upper Bound on the Estimation Error of uPU+TRADES]
\label{thm:uputr-estimation}

Let a function class $\mathscr{G}$ be given.
Assume the following (as in the previous subsection):

\begin{itemize}
\item \textbf{(Boundedness of the loss)} There exist constants $C_\ell, C_{\mathrm{KL}}>0$ such that
for any $y\in\mathcal{Y}$ and any input, the following holds:
\begin{itemize}
\item (Classification loss) One of the following holds:
\begin{itemize}
\item There exists a constant $C_\ell>0$ such that for any $\widehat y\in\mathbb{R}$,
$\ell(\widehat y,y)\le C_\ell$, or
\item there exists a constant $C_g>0$ such that $\|g\|_\infty\le C_g$ (for $g\in\mathscr{G}$) and
for $|\widehat y|\le C_g$, $\ell(\widehat y,y)\le C_\ell$.
\end{itemize}
\item (TRADES term) For any $u,v$,
$\ell_{\mathrm{KL}}(u,v)\le C_{\mathrm{KL}}$．
\end{itemize}

\item \textbf{(Lipschitz continuity)} There exist constants $L_\ell, L_{\mathrm{KL}}>0$ such that
\begin{itemize}
\item (Classification loss) $\ell(\widehat y,y)$ is $L_\ell$-Lipschitz continuous with respect to $\widehat y$.
\item (TRADES term) $\ell_{\mathrm{KL}}(u,v)$ is $L_{\mathrm{KL}}$-Lipschitz continuous in each argument
\end{itemize}
\end{itemize}

Then, for any $\delta>0$,
the following holds with probability at least $1-\delta$:
\footnotesize
\begin{align}
R_{\mathrm{uPU\mbox{-}TR}}(\widehat g_{\mathrm{uPU\mbox{-}TR}})-R_{\mathrm{uPU\mbox{-}TR}}(g^*)
\ \le\
&
8\pi_{\mathrm P}\big(L_\ell+2\beta L_{\mathrm{KL}}\big)\mathfrak{R}_{n_{\mathrm P},p_{\mathrm P}}(\mathscr G) \nonumber\\
& +
4\big(L_\ell+4\beta L_{\mathrm{KL}}\big)\mathfrak{R}_{n_{\mathrm U},p_{\mathrm U}}(\mathscr G)
\nonumber\\
&+
8\beta L_{\mathrm{KL}}\,\varepsilon W d^{1/q}
\left(
\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}
+\frac{1}{\sqrt{n_{\mathrm U}}}
\right)
\nonumber\\
&+
\sqrt{2\ln\frac{2}{\delta}}
\left(
\frac{\pi_{\mathrm P}(2C_\ell+\beta C_{\mathrm{KL}})}{\sqrt{n_{\mathrm P}}}
+
\frac{C_\ell+\beta C_{\mathrm{KL}}}{\sqrt{n_{\mathrm U}}}
\right).
\label{eq:uputr-estimation-bound}
\end{align}
\end{thm}
\end{tcolorbox}

\paragraph{\textbf{Interpretation and implications (statistical convergence rate)}}
Under the linear-in-parameters model ($\|\bm w\|_p\le W$) and bounded inputs, standard bounds in Appendix~\ref{app:toolbox} yield
\[
\mathfrak{R}_{n_{\mathrm P}, p_{\mathrm P}}(\mathscr G)=\mathcal{O}\!\left(\frac{W}{\sqrt{n_{\mathrm P}}}\right),\qquad
\mathfrak{R}_{n_{\mathrm U}, p_{\mathrm U}}(\mathscr G)=\mathcal{O}\!\left(\frac{W}{\sqrt{n_{\mathrm U}}}\right)
\]
Substituting this bound into \eqref{eq:uputr-estimation-bound} and absorbing constants such as $\beta$ and $\varepsilon$, the estimation-error upper bound becomes
\[
R_{\mathrm{uPU\mbox{-}TR}}(\widehat g_{\mathrm{uPU\mbox{-}TR}})-R_{\mathrm{uPU\mbox{-}TR}}(g^*)
=
\mathcal{O}_p\!\left(\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}+\frac{1}{\sqrt{n_{\mathrm U}}}\right)
\]
This yields an upper bound that converges to $0$ in probability as $n_{\mathrm P},n_{\mathrm U}\to\infty$.

As preparation for Theorem~\ref{thm:uputr-estimation},
we present a lemma bounding the uniform deviation for uPU+TRADES.


\paragraph{Auxiliary lemmas}
Below we use the auxiliary
Lemma~\ref{lem:advRad} (Rademacher increase under adversarial inputs) and
Lemma~\ref{lem:vecContract} (vector contraction).


\begin{tcolorbox}[
    colback = white,
    colframe = black!60,
    boxrule = 0.35pt,
    fonttitle = \bfseries,
    breakable = false]
\begin{lem}[Upper Bound on the Uniform Deviation for uPU+TRADES]
\label{lem:uputr-unif-jp}
For any $\delta>0$, with probability at least $1-\delta$,
\small
\begin{align}
\sup_{g\in\mathscr G}\left|\widehat R_{\mathrm{uPU\mbox{-}TR}}(g)-R_{\mathrm{uPU\mbox{-}TR}}(g)\right|
\ \le\
&
4\pi_{\mathrm P}\big(L_\ell+2\beta L_{\mathrm{KL}}\big)\mathfrak{R}_{n_{\mathrm P},p_{\mathrm P}}(\mathscr G) \nonumber\\
& +
2\big(L_\ell+4\beta L_{\mathrm{KL}}\big)\mathfrak{R}_{n_{\mathrm U},p_{\mathrm U}}(\mathscr G)
\nonumber\\
&+
4\beta L_{\mathrm{KL}}\,\varepsilon W d^{1/q}
\left(
\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}
+\frac{1}{\sqrt{n_{\mathrm U}}}
\right)
\nonumber\\
&+
\sqrt{\frac{1}{2}\ln\frac{2}{\delta}}
\left(
\frac{\pi_{\mathrm P}(2C_\ell+\beta C_{\mathrm{KL}})}{\sqrt{n_{\mathrm P}}}
+
\frac{C_\ell+\beta C_{\mathrm{KL}}}{\sqrt{n_{\mathrm U}}}
\right)
\label{eq:uputr-unif-bound-jp}
\end{align}
\end{lem}
\end{tcolorbox}


\par\noindent\textit{(Proof is given in Appendix~\ref{app:proof-lem-uputr-unif-jp}.)}



\par\noindent\textit{(Proof is given in Appendix~\ref{app:proof-thm-uputr-estimation}.)}



%============================================================
% nnPU+TRADES
%============================================================

\subsection{Upper Bound on the Estimation Error of nnPU+TRADES}
\label{subsec:nnpu-trades-bound}

In this section, we derive an upper bound on the estimation error of nnPU+TRADES.

\paragraph{nnPU+TRADES risk (population risk and empirical risk)}
First, define the adversarial regularization term in TRADES as
\[
\psi(g,\bm x)
:=
\max_{\|\bm\eta\|_{\infty}\le \varepsilon}
\ell_{\mathrm{KL}}\big(g(\bm x+\bm\eta),\,g(\bm x)\big)
\]
 .
We define the population risk of nnPU+TRADES by adding this regularization term to the nnPU (Kiryo et al., 2017) risk estimator:
\begin{align}
R_{\mathrm{nnPU\mbox{-}TR}}(g)
:={}&
\pi_{\mathrm P}\,\mathbb{E}_{\mathrm P}\!\left[\ell\big(g(\bm x),+1\big)+\beta\,\psi(g,\bm x)\right]
\nonumber\\
&+
\max\Bigl\{0,\,-\pi_{\mathrm P}\,\mathbb{E}_{\mathrm P}\!\left[\ell\big(g(\bm x),-1\big)\right]
+\mathbb{E}_{\mathrm U}\!\left[\ell\big(g(\bm x),-1\big)\right]\Bigr\}
\nonumber\\
&+\beta\,\mathbb{E}_{\mathrm U}\!\left[\psi(g,\bm x)\right]
\label{eq:nnputr-true-risk-jp}
\end{align}
as above.
The corresponding empirical risk is
\begin{align}
\widehat R_{\mathrm{nnPU\mbox{-}TR}}(g)
:={}&
\frac{\pi_{\mathrm P}}{n_{\mathrm P}}\sum_{i=1}^{n_{\mathrm P}}
\Bigl[\ell\big(g(\bm x_i^{\mathrm P}),+1\big)+\beta\,\psi(g,\bm x_i^{\mathrm P})\Bigr]
\nonumber\\
&+
\max\Biggl\{0,\,-\frac{\pi_{\mathrm P}}{n_{\mathrm P}}\sum_{i=1}^{n_{\mathrm P}}\ell\big(g(\bm x_i^{\mathrm P}),-1\big)
+\frac{1}{n_{\mathrm U}}\sum_{i=1}^{n_{\mathrm U}}\ell\big(g(\bm x_i^{\mathrm U}),-1\big)\Biggr\}
\nonumber\\
&+\frac{\beta}{n_{\mathrm U}}\sum_{i=1}^{n_{\mathrm U}}\psi(g,\bm x_i^{\mathrm U})
\label{eq:nnputr-emp-risk-jp}
\end{align}
 .
Define the empirical risk minimizer as
\[
\widehat g_{\mathrm{nnPU\mbox{-}TR}}
:=\arg\min_{g\in\mathscr G}\widehat R_{\mathrm{nnPU\mbox{-}TR}}(g)
\]
Also, let
\[
 g^*\in\arg\min_{g\in\mathscr G} R_{\mathrm{nnPU\mbox{-}TR}}(g)
\]
be a population risk minimizer.


\begin{tcolorbox}[
    colback = white,
    colframe = black,
    fonttitle = \bfseries,
    breakable = false]
\begin{thm}[Upper Bound on the Estimation Error of nnPU+TRADES]
\label{thm:nnputr-estimation}

Let a function class $\mathscr{G}$ be given.
Assume the following (as in the previous subsection):

\begin{itemize}
\item \textbf{(Boundedness of the loss)} There exist constants $C_\ell, C_{\mathrm{KL}}>0$ such that
for any $y\in\mathcal{Y}$ and any input, the following holds:
\begin{itemize}
\item (Classification loss) One of the following holds:
\begin{itemize}
\item There exists a constant $C_\ell>0$ such that for any $\widehat y\in\mathbb{R}$,
$\ell(\widehat y,y)\le C_\ell$, or
\item there exists a constant $C_g>0$ such that $\|g\|_\infty\le C_g$ (for $g\in\mathscr{G}$) and
for $|\widehat y|\le C_g$, $\ell(\widehat y,y)\le C_\ell$.
\end{itemize}
\item (TRADES term) For any $u,v$,
$\ell_{\mathrm{KL}}(u,v)\le C_{\mathrm{KL}}$．
\end{itemize}

\item \textbf{(Lipschitz continuity)} There exist constants $L_\ell, L_{\mathrm{KL}}>0$ such that
\begin{itemize}
\item (Classification loss) $\ell(\widehat y,y)$ is $L_\ell$-Lipschitz continuous with respect to $\widehat y$.
\item (TRADES term) $\ell_{\mathrm{KL}}(u,v)$ is $L_{\mathrm{KL}}$-Lipschitz continuous in each argument
\end{itemize}
\end{itemize}

Then, for any $\delta>0$,
the following holds with probability at least $1-\delta$:
\footnotesize
\begin{align}
R_{\mathrm{nnPU\mbox{-}TR}}(\widehat g_{\mathrm{nnPU\mbox{-}TR}})-R_{\mathrm{nnPU\mbox{-}TR}}(g^*)
\ \le\
&
8\pi_{\mathrm P}\big(L_\ell+2\beta L_{\mathrm{KL}}\big)\mathfrak{R}_{n_{\mathrm P},p_{\mathrm P}}(\mathscr G)
\nonumber\\
&+
4\big(L_\ell+4\beta L_{\mathrm{KL}}\big)\mathfrak{R}_{n_{\mathrm U},p_{\mathrm U}}(\mathscr G)
\nonumber\\
&+
8\beta L_{\mathrm{KL}}\,\varepsilon W d^{1/q}
\left(
\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}
+\frac{1}{\sqrt{n_{\mathrm U}}}
\right)
\nonumber\\
&+
\sqrt{2\ln\frac{2}{\delta}}
\left(
\frac{\pi_{\mathrm P}(2C_\ell+\beta C_{\mathrm{KL}})}{\sqrt{n_{\mathrm P}}}
+
\frac{C_\ell+\beta C_{\mathrm{KL}}}{\sqrt{n_{\mathrm U}}}
\right).
\label{eq:nnputr-estimation-bound}
\end{align}
\end{thm}
\end{tcolorbox}

\paragraph{\textbf{Interpretation and implications (statistical convergence rate)}}
Under the linear-in-parameters model ($\|\bm w\|_p\le W$) and bounded inputs, standard bounds in Appendix~\ref{app:toolbox} yield
\[
\mathfrak{R}_{n_{\mathrm P}, p_{\mathrm P}}(\mathscr G)=\mathcal{O}\!\left(\frac{W}{\sqrt{n_{\mathrm P}}}\right),\qquad
\mathfrak{R}_{n_{\mathrm U}, p_{\mathrm U}}(\mathscr G)=\mathcal{O}\!\left(\frac{W}{\sqrt{n_{\mathrm U}}}\right)
\]
Substituting this bound into \eqref{eq:nnputr-estimation-bound} and absorbing constants such as $\beta$ and $\varepsilon$, the estimation-error upper bound becomes
\[
R_{\mathrm{nnPU\mbox{-}TR}}(\widehat g_{\mathrm{nnPU\mbox{-}TR}})-R_{\mathrm{nnPU\mbox{-}TR}}(g^*)
=
\mathcal{O}_p\!\left(\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}+\frac{1}{\sqrt{n_{\mathrm U}}}\right)
\]
This yields an upper bound that converges to $0$ in probability as $n_{\mathrm P},n_{\mathrm U}\to\infty$.

As preparation for Theorem~\ref{thm:nnputr-estimation}, we provide a lemma bounding the uniform deviation for nnPU+TRADES.


\paragraph{Auxiliary lemmas}
Below we use the auxiliaryLemma~\ref{lem:advRad} (Rademacher increase under adversarial inputs) and
Lemma~\ref{lem:vecContract} (vector contraction).


\begin{tcolorbox}[
    colback = white,
    colframe = black!60,
    boxrule = 0.35pt,
    fonttitle = \bfseries,
    breakable = false]
\begin{lem}[Upper Bound on the Uniform Deviation for nnPU+TRADES]
\label{lem:nnputr-unif-jp}
For any $\delta>0$, with probability at least $1-\delta$,
\footnotesize
\begin{align}
\sup_{g\in\mathscr G}\left|\widehat R_{\mathrm{nnPU\mbox{-}TR}}(g)-R_{\mathrm{nnPU\mbox{-}TR}}(g)\right|
\ \le\
&
4\pi_{\mathrm P}\big(L_\ell+2\beta L_{\mathrm{KL}}\big)\mathfrak{R}_{n_{\mathrm P},p_{\mathrm P}}(\mathscr G)
\nonumber\\
&+
2\big(L_\ell+4\beta L_{\mathrm{KL}}\big)\mathfrak{R}_{n_{\mathrm U},p_{\mathrm U}}(\mathscr G)
\nonumber\\
&+
4\beta L_{\mathrm{KL}}\,\varepsilon W d^{1/q}
\left(
\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}
+\frac{1}{\sqrt{n_{\mathrm U}}}
\right)
\nonumber\\
&+
\sqrt{\frac{1}{2}\ln\frac{2}{\delta}}
\left(
\frac{\pi_{\mathrm P}(2C_\ell+\beta C_{\mathrm{KL}})}{\sqrt{n_{\mathrm P}}}
+
\frac{C_\ell+\beta C_{\mathrm{KL}}}{\sqrt{n_{\mathrm U}}}
\right).
\label{eq:nnputr-unif-bound-jp}
\end{align}
\end{lem}
\end{tcolorbox}


\par\noindent\textit{(Proof is given in Appendix~\ref{app:proof-lem-nnputr-unif-jp}.)}




\par\noindent\textit{(Proof is given in Appendix~\ref{app:proof-thm-nnputr-estimation}.)}


\clearpage
\subsection{Sufficient Unlabeled Sample Size for PU+TRADES to Outperform Supervised TRADES}
\label{subsec:nu_condition_trades}

In this section, we compare the estimation-error upper bounds obtained in the previous subsections (Theorems~\ref{thm:pntr-estimation}, \ref{thm:uputr-estimation}, and \ref{thm:nnputr-estimation}) and derive sufficient conditions on the unlabeled sample size $n_{\mathrm U}$ under which the bound for supervised TRADES (PN+TRADES) becomes larger than that for PU+TRADES (uPU+TRADES / nnPU+TRADES), i.e., conditions under which PU+TRADES can theoretically outperform supervised TRADES.

\paragraph{Rademacher complexity of the linear hypothesis class}
Under the assumptions in this chapter ($\|\bm x\|_\infty\le C_x$ and $\|\bm w\|_p\le W$),
for any distribution $\nu$,
\begin{align}
\mathfrak{R}_{n,\nu}(\mathscr G)
=\mathbb E_{\bm x_{1:n}\sim\nu}\,\mathbb E_{\bm\sigma}
\Big[\sup_{\|\bm w\|_p\le W}\frac{1}{n}\sum_{i=1}^n\sigma_i\bm w^\top\bm x_i\Big]
\ \le\ \frac{W\,\sup_{\bm x\in\mathcal X}\|\bm x\|_q}{\sqrt n}
\ \le\ \frac{W C_x\,d^{1/q}}{\sqrt n}
\label{eq:rad-linear-basic}
\end{align}
holds (where $1/p+1/q=1$).
In the following, for notational clarity,
\begin{equation}
\kappa_\delta:=\sqrt{2\ln\frac{2}{\delta}}
\end{equation}
and we rewrite the estimation-error bounds into a $1/\sqrt{n}$ form for comparison.
Moreover, to collect constant factors,
\begin{equation}
\Gamma_\delta
:=
4\big(L_\ell+4\beta L_{\mathrm{KL}}\big)\,W C_x d^{1/q}
+
8\beta L_{\mathrm{KL}}\,\varepsilon W d^{1/q}
+
\kappa_\delta\,(C_\ell+\beta C_{\mathrm{KL}})
\label{eq:Gamma-delta-def}
\end{equation}
 .

\paragraph{(1) Comparing PN+TRADES and uPU+TRADES}
First, applying \eqref{eq:rad-linear-basic} to Theorem~\ref{thm:pntr-estimation} gives, with probability at least $1-\delta$,
\begin{align}
R_{\mathrm{PN\mbox{-}TR}}(\widehat g_{\mathrm{PN\mbox{-}TR}})-R_{\mathrm{PN\mbox{-}TR}}(g^*)
\ \le\
\Gamma_\delta\Big(\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}+\frac{\pi_{\mathrm N}}{\sqrt{n_{\mathrm N}}}\Big)
\label{eq:pntr-bound-simplified}
\end{align}
is obtained.

Similarly, applying \eqref{eq:rad-linear-basic} to Theorem~\ref{thm:uputr-estimation} yields, with probability at least $1-\delta$,
\begin{align}
R_{\mathrm{uPU\mbox{-}TR}}(\widehat g_{\mathrm{uPU\mbox{-}TR}})-R_{\mathrm{uPU\mbox{-}TR}}(g^*)
\ \le\
\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}\Big(\Gamma_\delta+4L_\ell\,W C_x d^{1/q}+\kappa_\delta C_\ell\Big)
+\frac{\Gamma_\delta}{\sqrt{n_{\mathrm U}}}
\label{eq:uputr-bound-simplified}
\end{align}
follows.

\paragraph{Main result: sufficient unlabeled sample size}
The following theorem, based on the comparison of the estimation-error upper bounds,
provides a sufficient condition on
the unlabeled sample size $n_{\mathrm U}$ under which PU+TRADES can outperform supervised TRADES.

\begin{tcolorbox}[
    colback = white,
    colframe = black,
    fonttitle = \bfseries,
    breakable = false]
\begin{thm}[Sufficient Unlabeled Sample Size for PU+TRADES to Outperform Supervised TRADES]
\label{thm:nu-threshold-trades}
We compare the estimation-error upper bounds in Theorems~\ref{thm:pntr-estimation}, \ref{thm:uputr-estimation}, and \ref{thm:nnputr-estimation}.
Using \eqref{eq:rad-linear-basic}, we reduce them to $1/\sqrt{n}$-type forms,
and we use $\Gamma_\delta$ and $\kappa_\delta$ defined in \eqref{eq:Gamma-delta-def}.
Then
\begin{equation}
\Gamma_\delta\frac{\pi_{\mathrm N}}{\sqrt{n_{\mathrm N}}}
\ >\
\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}\Big(4L_\ell\,W C_x d^{1/q}+\kappa_\delta C_\ell\Big)
\label{eq:compare-pn-uputr-feasible}
\end{equation}
assume it holds.

\begin{enumerate}
\item[(i)] （PN+TRADES vs uPU+TRADES）
If
\begin{equation}
\boxed{\
\ n_{\mathrm U}
\ >\
\left(
\frac{\Gamma_\delta}
{\Gamma_\delta\frac{\pi_{\mathrm N}}{\sqrt{n_{\mathrm N}}}
-\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}\big(4L_\ell\,W C_x d^{1/q}+\kappa_\delta C_\ell\big)}
\right)^{\!2}
\ }
\label{eq:nu-threshold-uputr}
\end{equation}
as a comparison of the estimation-error upper bounds,
the bound for PN+TRADES (\eqref{eq:pntr-bound-simplified})
is larger than that for uPU+TRADES (\eqref{eq:uputr-bound-simplified}).

\item[(ii)] （PN+TRADES vs nnPU+TRADES）
Since the right-hand side of Theorem~\ref{thm:nnputr-estimation} is identical to that of Theorem~\ref{thm:uputr-estimation},
the same sufficient condition
\begin{equation}
\boxed{\
\ n_{\mathrm U}
\ >\
\left(
\frac{\Gamma_\delta}
{\Gamma_\delta\frac{\pi_{\mathrm N}}{\sqrt{n_{\mathrm N}}}
-\frac{\pi_{\mathrm P}}{\sqrt{n_{\mathrm P}}}\big(4L_\ell\,W C_x d^{1/q}+\kappa_\delta C_\ell\big)}
\right)^{\!2}
\ }
\label{eq:nu-threshold-nnputr}
\end{equation}
implies that the PN+TRADES estimation-error upper bound is larger than the nnPU+TRADES one.
\end{enumerate}
\end{thm}
\end{tcolorbox}


\par\noindent\textit{(Proof is given in Appendix~\ref{app:proof-thm-nu-threshold-trades}.)}


If condition \eqref{eq:compare-pn-uputr-feasible} does not hold,
the denominator on the right-hand side becomes non-positive, and thus there is no $n_{\mathrm U}$ satisfying the comparison inequality in (i);
within the scope of comparisons based on the bounds derived in this chapter,
we cannot claim that PU+TRADES outperforms supervised TRADES.
On the other hand, when \eqref{eq:compare-pn-uputr-feasible} holds,
by \eqref{eq:nu-threshold-uputr} and \eqref{eq:nu-threshold-nnputr},
in the regime where the unlabeled sample size is sufficiently large,
the superiority of PU+TRADES is theoretically guaranteed.


